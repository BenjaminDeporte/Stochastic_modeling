{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d87cdad0",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 40px; color: black; font-weight: bold;\">\n",
    "Notebook for NumPyro implementation of learning an Ornstein Uhlenbeck process\n",
    "</div>\n",
    "\n",
    "(Ref : ch11. from Sarkka and Solin \"Applied Stochastic Differential Equations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fccee44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "import torchsde\n",
    "\n",
    "import jax.numpy as jnp\n",
    "import jax\n",
    "import numpyro\n",
    "import numpyro.distributions as ndist\n",
    "from numpyro.contrib.control_flow import scan\n",
    "from numpyro.infer import MCMC, NUTS\n",
    "from numpyro.diagnostics import summary\n",
    "\n",
    "from numpyro_models import numpyro_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f3154024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=42):\n",
    "    \"\"\"\n",
    "    Set seed for reproducibility.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)  # if you are using multi-GPU.\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f909c29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n",
      "GPU Name: NVIDIA GeForce RTX 3080 Ti\n",
      "Total GPU Memory: 11.8 GB\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    # if using a GPU (CUDA) and num_chains > 1\n",
    "    # torch.multiprocessing.set_start_method('spawn', force=True)\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "# NB Warning : setting the default_device to CUDA creates a device conflict\n",
    "# when using a DataLoader, as it uses a CPU-generator for shuffling\n",
    "torch.set_default_device(device)\n",
    "print(f\"Using {device}\")\n",
    "\n",
    "torch.set_default_dtype(torch.float32)\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print('GPU Name:', torch.cuda.get_device_name(0))\n",
    "    print('Total GPU Memory:', round(torch.cuda.get_device_properties(0).total_memory/1024**3,1), 'GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359140b2",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 40px; color: black; font-weight: bold;\">\n",
    "0 : Create sample path from Ornstein-Uhlenbeck\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe9eeb6",
   "metadata": {},
   "source": [
    "\\begin{align}\n",
    "dX_t &= - \\lambda X_t dt + \\sigma dB_t \\\\\n",
    "X_0 &= x_0\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb8d876",
   "metadata": {},
   "source": [
    "### Ground Truth Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "26ea4c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_gt = 1.0  # Rate of mean reversion\n",
    "sigma_gt = 1.0  # Volatility parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b70b42b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OU_SDE with parameter lambda = 1.00e+00 and sigma = 1.00e+00\n"
     ]
    }
   ],
   "source": [
    "# create an Ornstein Uhlenbeck SDE model:\n",
    "# dX_t = theta * (mu - X_t) dt + sigma dW_t\n",
    "# theta, mu, sigma scalar parameters\n",
    "# W_t is a standard 1D Brownian motion\n",
    "\n",
    "# SDE are instantiated as subclasses of nn.Module\n",
    "\n",
    "class OUSDE(nn.Module):\n",
    "    def __init__(self, lambda_, sigma):\n",
    "        \n",
    "        # noise type can take 4 values : \"diagonal\", \"general\", \"additive\", \"scalar\"\n",
    "        # here we use \"diagonal\" : the diffusion function g(t,y) is an element wise function,\n",
    "        # its output has the same shape as y, ie (batch_size, state_size)\n",
    "        \n",
    "        # sde_type can be \"ito\" or \"stratonovich\"\n",
    "        # we use \"ito\" here. The available methods for computation are Euler(-Maruyama), Milstein, SRK.\n",
    "        super().__init__()\n",
    "        self.noise_type = \"diagonal\"\n",
    "        self.sde_type = \"ito\"\n",
    "        \n",
    "        # we register the parameters so we can save them. But we will not train them.\n",
    "        self.register_buffer(\"lambda_\", torch.tensor(lambda_))\n",
    "        self.register_buffer(\"sigma\", torch.tensor(sigma))\n",
    "\n",
    "    # DRIFT FUNCTION\n",
    "    # inputs are:\n",
    "    # - t : a tensor of shape (1,) representing the time stamps\n",
    "    # - y : a tensor of shape (batch_size, state_size) representing the current state\n",
    "    # outputs:\n",
    "    # - a tensor of shape (batch_size, state_size) representing the drift at time t and state y\n",
    "    # note : the functions f and g must be able to handle inputs of shape (batch_size, state_size)\n",
    "    # for any batch_size >= 1\n",
    "    def f(self, t, y):\n",
    "        return -self.lambda_ * y\n",
    "    \n",
    "    # DIFFUSION FUNCTION\n",
    "    # inputs are:\n",
    "    # - t : a tensor of shape (1,) representing the time stamps\n",
    "    # - y : a tensor of shape (batch_size, state_size) representing the current state\n",
    "    # outputs:\n",
    "    # - a tensor of shape (batch_size, state_size) representing the diffusion at time t and state y\n",
    "    # (NB : generally, the output of g is of shape (batch_size, state_size, brownian_size) when noise_type is \"general\")\n",
    "    def g(self, t, y):\n",
    "        return self.sigma * torch.ones_like(y)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        msg = f'OU_SDE with parameter lambda = {self.lambda_:.2e} and sigma = {self.sigma:.2e}'\n",
    "        return msg\n",
    "    \n",
    "model = OUSDE(lambda_=lambda_gt, sigma=sigma_gt).to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "043ace79",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_POINTS = 100 # number of points in each path\n",
    "N_PATHS = 1 # number of paths to sample\n",
    "\n",
    "t_start = 0.0\n",
    "t_end = 10.0\n",
    "\n",
    "ts = torch.linspace(t_start, t_end, N_POINTS).to(device)  # time stamps where we want the solution, between 0 and 1\n",
    "print(f\"Time stamps shape : {ts.shape}\")  # shape (N_POINTS,)\n",
    "y_start = 0.0  # initial condition\n",
    "y0 = torch.full((N_PATHS, 1), y_start).to(device)  # initial condition 0.25, shape (batch_size=N_PATHS, state_size=1)\n",
    "print(f\"Initial condition shape : {y0.shape}\")  # shape (N_PATHS, 1)\n",
    "\n",
    "# now, we call sdeint to solve the SDE\n",
    "# NB : we can use the adjoint method by calling sdeint_adjoint instead of sdeint\n",
    "# method can be \"euler\", \"milstein\", \"srk\" for sde_type=\"ito\"\n",
    "# dt is the step size used by the solver (smaller dt -> more accurate but slower). By default, dt=1e-3\n",
    "\n",
    "with torch.no_grad():  # we don't need gradients for this demo\n",
    "    ys = torchsde.sdeint(model, y0, ts, method=\"euler\", dt=1e-3)  # shape (N_POINTS, N_PATHS, 1)\n",
    "print(f\"Computed solution samples : {ys.size()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d75acc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check on the time scale\n",
    "\n",
    "print(f\"Delta t = {(t_end - t_start) / N_POINTS:.3e}\")\n",
    "print(f\"Time constant (1/lambda) = {1/ lambda_gt:.3e}\")\n",
    "\n",
    "ldt = (t_end - t_start) / N_POINTS * lambda_gt\n",
    "print(f\"lambda . Delta_t = {ldt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07f1d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ousde_samples(ts, ys, model, title=None):\n",
    "    \"\"\"\n",
    "    Utility functions to plot the sampled SDE solutions\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(10,6))\n",
    "    n_points = ts.size()[0]\n",
    "    n_paths = ys.size()[1]\n",
    "    \n",
    "    for i, y in enumerate(ys.permute(1,0,2)):  # iterate over paths\n",
    "        ax.plot(ts.detach().cpu().numpy(), y.detach().cpu().numpy(), lw=1, alpha=1.0, label=f'Path {i+1}' if i<10 else None)  # plot each path\n",
    "    \n",
    "    if title is None:\n",
    "        title = f\"DATA : {n_paths} Sampled path(s) of the O.U. SDE ({n_points:.0f} points) - parameters : lambda = {model.lambda_.item():.1e}, sigma = {model.sigma.item():.1e}\"\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Time\")\n",
    "    ax.set_ylabel(\"X(t)\")\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "        \n",
    "    return fig, ax\n",
    "\n",
    "fig, ax = plot_ousde_samples(ts, ys, model)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee89d81",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 40px; color: black; font-weight: bold;\">\n",
    "1 : Maximum Likelihood (Point Estimates)\n",
    "</div>\n",
    "\n",
    "### Analytical Solution for Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ae2063",
   "metadata": {},
   "source": [
    "Analytical solution:\n",
    "\n",
    "\\begin{align}\n",
    "X_t &= e^{-\\lambda t}X_0 + \\sigma \\int_{0}^{t}e^{\\lambda (s-t)}dB_s \\\\\n",
    "X_0 &= x_0\n",
    "\\end{align}\n",
    "\n",
    "Therefore:\n",
    "\\begin{align}\n",
    "X_t \\vert X_s \\sim \\mathcal{N}\\left( e^{-\\lambda (t-s)}X_s \\,;\\, \\frac{\\sigma^2}{2 \\lambda} (1 - e^{-2 \\lambda (t-s)})\\right)\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e040a3f7",
   "metadata": {},
   "source": [
    "Likelihood (assuming constant time intervals : $\\Delta t = t_{k+1}-t_k$)\n",
    "\n",
    "\\begin{align}\n",
    "- \\log{p(x_{1:N})} &= \\frac{N}{2} \\log{\\Sigma} + \\frac{1}{2 \\Sigma} \\sum_{k=0}^{N-1} (x_{k+1} - a x_k)^2 + \\text{cte}\n",
    "\\end{align}\n",
    "with:\n",
    "\\begin{align}\n",
    "a &= e^{-\\lambda \\Delta t} \\\\\n",
    "\\Sigma &= \\frac{\\sigma^2}{2 \\lambda} (1-e^{-2 \\lambda \\Delta t})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd06888",
   "metadata": {},
   "source": [
    "MLE:\n",
    "\n",
    "\\begin{align}\n",
    "\\lambda_{ML} &= - \\frac{1}{\\Delta t} \\log{\\frac{\\sum_{k=0}^{T-1} x_k x_{k+1}}{\\sum_{k=0}^{T-1} x_k^2}} \\\\\n",
    "\\sigma_{ML}^2 &= \\frac{1}{N} \\left( \\frac{2 \\lambda_{ML}}{1 - \\exp{(-2 \\lambda_{ML} \\Delta t)}} \\right) \\sum_{k=0}^{T-1} \\left( x_{k+1} - \\exp{(-\\lambda_{ML} \\Delta t)} x_k\\right) ^2\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c270d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_t = (t_end - t_start) / (N_POINTS - 1)  # time step between points\n",
    "\n",
    "for i, x in enumerate(ys.permute(1,0,2)):  # iterate over paths\n",
    "    # get path data\n",
    "    x_ = x.squeeze(-1)  # shape (N_POINTS,)\n",
    "    # compute theta ML estimate\n",
    "    s1 = np.sum(x_[:-1].cpu().numpy() * x_[1:].cpu().numpy())  # sum over time points\n",
    "    s2 = np.sum(x_[:-1].cpu().numpy()**2)  # sum over time points\n",
    "    lambda_ml = - (1/delta_t) * np.log(s1 / s2)\n",
    "    print(f\"Estimated lambda ML for path {i+1} : {lambda_ml:.3e} vs ground truth : {lambda_gt:.3e}\")\n",
    "    # autre méthode to check\n",
    "    s3 = np.sum([x_[k].cpu().numpy()*x_[k+1].cpu().numpy() for k in range(x_.shape[0]-1) ])\n",
    "    s4 = np.sum([x_[k].cpu().numpy()**2 for k in range(x_.shape[0]-1)])\n",
    "    lambda_est_2 = -1/(delta_t)*np.log(s3/s4)\n",
    "    # print(f\"Autre méthode : lambda_est = {lambda_est_2:.4e}\")\n",
    "    # compute sigma ML estimate\n",
    "    s3 = np.sum((x_[1:].cpu().numpy() - np.exp(-lambda_ml * delta_t) * x_[:-1].cpu().numpy())**2)  # sum over time points\n",
    "    sigma_ml = np.sqrt(s3 / N_POINTS / (1 - np.exp(-2 * lambda_ml * delta_t)) * 2 * lambda_ml)\n",
    "    print(f\"Estimated sigma ML for path {i+1} : {sigma_ml:.3e} vs ground truth : {sigma_gt:.3e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08aa1c6c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 40px; color: black; font-weight: bold;\">\n",
    "Using NumPyro\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a15be98",
   "metadata": {},
   "source": [
    "We put uninformative priors on $\\lambda > 0, \\sigma > 0$ and $x_0$\n",
    "\n",
    "\\begin{align}\n",
    "\\log{\\lambda} &\\sim \\mathcal{N}(0.0, 1.0) \\\\\n",
    "\\log{\\sigma} &\\sim \\mathcal{N}(0.0, 1.0) \\\\\n",
    "x_0 &\\sim \\mathcal{N}(0.0, 1.0)\n",
    "\\end{align}\n",
    "\n",
    "and compute the likelihood as (seen above, but this time allowing for changing time intervals):\n",
    "\n",
    "\\begin{align}\n",
    "X_{t_{k+1}} \\vert X_{t_k} &\\sim \\mathcal{N}\\left( e^{-\\lambda (t_{k+1}-t_k)}X_{t_k} ; \\frac{\\sigma^2}{2 \\lambda} (1 - e^{-2 \\lambda (t_{k+1}-t_k)})\\right) \\\\\n",
    "p(x_{k+1} \\vert x_k) &= \\frac{1}{\\sqrt{2\\pi \\sigma_k^2}} \\exp{\\left( -\\frac{1}{\\sigma_k^2}(x_{k+1} - a_k x_k)^2 \\right) }\n",
    "\\end{align}\n",
    "\n",
    "Which leads to:\n",
    "\n",
    "\\begin{align}\n",
    "\\log{p(x_{1:N})} &= -\\frac{1}{2} \\sum_{k=0}^{N-1} \\log{\\sigma_k^2} - \\frac{1}{2} \\sum_{k=0}^{N-1} \\frac{(x_{k+1} - a_k x_k)^2}{\\sigma_k^2} + \\log{p(x_0)} + \\text{cte}\n",
    "\\end{align}\n",
    "with:\n",
    "\\begin{align}\n",
    "a_k &= e^{-\\lambda (t_{k+1} - t_k)} \\\\\n",
    "\\sigma_k^2 &= \\Sigma_k = \\frac{\\sigma^2}{2 \\lambda} (1-e^{-2 \\lambda (t_{k+1} - t_k)})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b4caa2c",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "Model\n",
    "</div>\n",
    "\n",
    "NB : we declare the `model` as a function, not a class, as we aim at doing MCMC sampling with NumPyro in the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab9417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = ts\n",
    "X = ys.squeeze()\n",
    "\n",
    "print(f'Time stamps : {t.size()}')\n",
    "print(f'Values : {X.size()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e7ad2a",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "A first model with no observation noise (basically MLE)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd36dafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro_models import numpyro_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2912ce20",
   "metadata": {},
   "source": [
    "MCMC Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0684e7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel = NUTS(numpyro_model)\n",
    "\n",
    "MCMC_runner = MCMC(\n",
    "    nuts_kernel,\n",
    "    num_warmup=2000,\n",
    "    num_samples=5000,\n",
    "    num_chains=4,\n",
    "    chain_method=\"vectorized\"\n",
    ")\n",
    "\n",
    "MCMC_runner.run(\n",
    "    jax.random.PRNGKey(0),\n",
    "    t=jnp.array(t),\n",
    "    X=jnp.array(X)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ccfd72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_diag_MCMC(diags):\n",
    "\n",
    "    # Extract all 'n_eff' tensors for example : number of effective samples from the Markov chains\n",
    "    n_eff_list = []\n",
    "    for rv_name, rv_diagnostics in diags.items():\n",
    "        \n",
    "        # We only care about n_eff for the parameters we are sampling (weights, biases, sigma)\n",
    "        # Exclude auxiliary or internal diagnostics if they exist.\n",
    "        if 'n_eff' in rv_diagnostics and rv_name not in  ['_last_state', 'potential_energy']:\n",
    "            \n",
    "            # n_eff might be a tensor if the site is multidimensional (like a weight matrix)\n",
    "            n_eff_tensor = rv_diagnostics['n_eff']\n",
    "            \n",
    "            # Flatten the tensor and add all its elements to our list\n",
    "            n_eff_list.append(n_eff_tensor)\n",
    "\n",
    "    # Concatenate all n_eff tensors into a single 1D tensor\n",
    "    all_n_eff = np.array(n_eff_list)\n",
    "\n",
    "    # Compute the average and minimum ESS\n",
    "    average_n_eff = all_n_eff.mean().item()\n",
    "    min_n_eff = all_n_eff.min().item()\n",
    "\n",
    "    print(f\"Total number of stochastic parameters analyzed: {len(all_n_eff)}\")\n",
    "    print(f\"Overall Average Effective Sample Size (ESS): {average_n_eff:.2f}\")\n",
    "    print(f\"Minimum Effective Sample Size (ESS): {min_n_eff:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60e41ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check effective sample size, etc. by getting the dictionnary of results\n",
    "# The structure is: {'<RV_name>': {'n_eff': tensor, 'r_hat': tensor, ...}, ...}\n",
    "# diags = MCMC_runner.diagnostics()\n",
    "\n",
    "samples = MCMC_runner.get_samples(group_by_chain=True)\n",
    "\n",
    "diags = summary(samples)\n",
    "\n",
    "display_diag_MCMC(diags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3ab91f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get samples and analyse\n",
    "numpyro_posterior_samples = MCMC_runner.get_samples()\n",
    "\n",
    "def display_posterior_samples(numpyro_posterior_samples):\n",
    "\n",
    "    lambda_samples = np.array(numpyro_posterior_samples.get(\"log_lambda\"))\n",
    "    sigma_samples = np.array(numpyro_posterior_samples.get(\"log_sigma\"))\n",
    "\n",
    "    lambda_samples = np.exp(lambda_samples)\n",
    "    hist_counts, bin_edges = np.histogram(lambda_samples, bins=100)\n",
    "    lambda_map = bin_edges[hist_counts.argmax()]  # approximate MAP\n",
    "\n",
    "    sigma_samples = np.exp(sigma_samples)\n",
    "    hist_counts, bin_edges = np.histogram(sigma_samples, bins=100)\n",
    "    sigma_map = bin_edges[hist_counts.argmax()]  # approximate MAP\n",
    "\n",
    "    print(f'lambda :')\n",
    "    print(f'\\tground truth : \\t{lambda_gt:.3e}')\n",
    "    print(f'\\tMLE : \\t\\t{lambda_ml:.3e}')\n",
    "    print(f'\\tMAP : \\t\\t{lambda_map:.3e}')\n",
    "    print(f'sigma :')\n",
    "    print(f'\\tground truth : \\t{sigma_gt:.3e}')\n",
    "    print(f'\\tMLE : \\t\\t{sigma_ml:.3e}')\n",
    "    print(f'\\tMAP : \\t\\t{sigma_map:.3e}')\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(20,6), nrows=1, ncols=3)\n",
    "\n",
    "    sns.histplot(lambda_samples, bins=50, kde=True, label=f'lambda', ax=ax[0])\n",
    "    ymax=ax[0].get_ylim()[1]*1.1\n",
    "    ax[0].vlines(lambda_gt, ymin=0, ymax=ymax, color='green', label='ground truth')\n",
    "    ax[0].vlines(lambda_ml, ymin=0, ymax=ymax, color='blue', linestyles='--', label='MLE')\n",
    "    ax[0].vlines(lambda_map, ymin=0, ymax=ymax, color='black', linestyles='--', label='MAP')\n",
    "    ax[0].set_title(f'lambda')\n",
    "    ax[0].legend()\n",
    "    ax[0].grid()\n",
    "\n",
    "    sns.histplot(sigma_samples, bins=50, kde=True, label=f'sigma', ax=ax[1])\n",
    "    ymax=ax[1].get_ylim()[1]*1.1\n",
    "    ax[1].vlines(sigma_gt, ymin=0, ymax=ymax, color='green', label='ground truth')\n",
    "    ax[1].vlines(sigma_ml, ymin=0, ymax=ymax, color='blue', linestyles='--', label='MLE')\n",
    "    ax[1].vlines(sigma_map, ymin=0, ymax=ymax, color='black', linestyles='--', label='MAP')\n",
    "    ax[1].set_title(f'sigma')\n",
    "    ax[1].legend()\n",
    "    ax[1].grid()\n",
    "\n",
    "    ax[2].scatter(lambda_samples, sigma_samples, marker='.', alpha=0.5)\n",
    "    ax[2].scatter(lambda_gt, sigma_gt, marker='x', s=50.0, color='green', label='ground truth')\n",
    "    ax[2].scatter(lambda_ml, sigma_ml, marker='x', s=50.0, color='blue', label='MLE')\n",
    "    ax[2].scatter(lambda_map, sigma_map, marker='x', s=50.0, color='black', label='MAP')\n",
    "    ax[2].set_title(f'lambda v sigma')\n",
    "    ax[2].set_xlabel('lambda')\n",
    "    ax[2].set_ylabel('sigma')\n",
    "    ax[2].legend()\n",
    "    ax[2].grid()\n",
    "\n",
    "    fig.suptitle(f\"MCMC sampling for O.U. with lambda x delta_t = {ldt:.3e}\")\n",
    "\n",
    "    plt.show()\n",
    "    \n",
    "display_posterior_samples(numpyro_posterior_samples=numpyro_posterior_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d9dbac",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 20px; color: black; font-weight: bold;\">\n",
    "A note on whether or not posterior sampling is useful...\n",
    "</div>\n",
    "\n",
    "Recall that \n",
    "\n",
    "\\begin{align}\n",
    "dX_t &= - \\lambda X_t dt + \\sigma dB_t \\\\\n",
    "X_0 &= x_0\n",
    "\\end{align}\n",
    "\n",
    "The big take-away is that if\n",
    "\\begin{align}\n",
    "\\lambda \\Delta t << 1\n",
    "\\end{align}\n",
    "\n",
    "Then $(X_t)_{t \\geq 0}$ is close to:\n",
    "\\begin{align}\n",
    "dX_t &\\sim \\sigma dB_t \\\\\n",
    "X_0 &= x_0\n",
    "\\end{align}\n",
    "\n",
    "That is the observed $X_t$ is close to a **random walk** and it is very difficult to estimate $\\lambda$.\n",
    "\n",
    "The O.U. process becomes indistinguishable from a random walk at that time scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12cb157",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 60px; color: black; font-weight: bold;\">\n",
    "Take-aways\n",
    "</div>\n",
    "\n",
    "High-frequency ≠ high information for λ.\n",
    "\n",
    "If $\\lambda \\Delta_t << 1$, treat the path as approximately a Wiener process; focus on $\\sigma$\n",
    "\n",
    "To infer $\\lambda$ reliably, you need:\n",
    "- Larger $\\Delta t$, or\n",
    "- Longer total observation window, spanning multiple relaxation times - ie $\\lambda \\Delta_t \\geq 0.1, 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a61ff",
   "metadata": {},
   "source": [
    "<div style=\"background-color: #008B8B; padding: 15px; border-radius: 5px; font-size: 40px; color: black; font-weight: bold;\">\n",
    "Now, using the fact that the O.U. is a Gaussian Process\n",
    "</div>\n",
    "\n",
    "ie :\n",
    "\\begin{align}\n",
    "X_{1:N} &\\sim \\mathcal{N}(0, \\Sigma(\\lambda, \\sigma)) \\\\\n",
    "\\Sigma_{ij} &= \\frac{\\sigma^2}{2 \\lambda} \\exp{- \\lambda \\vert t_i - t_j \\vert}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b56f559",
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpyro_models import ou_gp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce57e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "nuts_kernel = NUTS(ou_gp_model)\n",
    "\n",
    "MCMC_runner = MCMC(\n",
    "    nuts_kernel,\n",
    "    num_warmup=2000,\n",
    "    num_samples=10000,\n",
    "    num_chains=4,\n",
    "    chain_method=\"vectorized\"\n",
    ")\n",
    "\n",
    "MCMC_runner.run(\n",
    "    jax.random.PRNGKey(0),\n",
    "    t=jnp.array(t),\n",
    "    X=jnp.array(X)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31798669",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check effective sample size, etc. by getting the dictionnary of results\n",
    "# The structure is: {'<RV_name>': {'n_eff': tensor, 'r_hat': tensor, ...}, ...}\n",
    "# diags = MCMC_runner.diagnostics()\n",
    "\n",
    "samples = MCMC_runner.get_samples(group_by_chain=True)\n",
    "\n",
    "diags = summary(samples)\n",
    "\n",
    "display_diag_MCMC(diags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "653e94f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_samples = {\n",
    "    k : v.reshape(-1,) for k,v in samples.items()\n",
    "}\n",
    "display_posterior_samples(numpyro_posterior_samples=flat_samples)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torchy",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
